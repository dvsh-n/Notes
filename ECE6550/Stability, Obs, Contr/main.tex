\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[a4paper, top=1in, bottom=1in, left=1in, right=1in]{geometry} % Adjust margins

\setlength{\parindent}{0pt} % Remove automatic indenting at new paragraphs
\setlength{\parskip}{\baselineskip} % Add a blank line between paragraphs

\title{Controllability and Observability}
\author{Devesh Nath}
\date{\today}

\begin{document}

\maketitle

\section{Controllability and Reachability}

\subsection{Controllability Definition}
A system is controllable if it is possible to drive the system from any initial state to any final state in a finite time, using an appropriate input sequence. 

\subsection{Reachability Definition}
A system is reachable if it is possible to drive the system from the origin to any final state in a finite time, using an appropriate input sequence.

\subsection{Controllability and Reachability Relationship}
An interval $I$ :=[$t_0,t_1$].

We define a linear mapping $\Gamma : U \rightarrow \mathbb{R}^n$ as follows:
\[
\Gamma(u) = \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) u(\tau) \, d\tau, \quad \forall u \in U
\]

$\Gamma$ is a mapping that maps a control u to state $\mathbb{R}^n$. 
The range of $\Gamma$, denoted by $\text{R}(\Gamma)$, is the set of all vectors $x \in \mathbb{R}^n$ for which there exists an control $u \in U$ such that $\Gamma(u) = x$. 

If $x_1$ is reachable, then there exists $u \in U$ such that:
\[
\int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) u(\tau) \, d\tau = x_1.
\]

$\text{Range}(\Gamma)$ is the space of reachable vectors.

\subsection{If completely reachable, then completely controllable}
First, suppose that the system is completely reachable. Then $\text{R}(\Gamma) = \mathbb{R}^n$. Given $x_0 \in \mathbb{R}^n$, there exists $u \in U$ such that
\[
\Gamma(u) = -\Phi(t_1, t_0)x_0.
\]
\textcolor{blue}{if $\Phi(t_1, t_0)x_0$ takes us to $x_1$, there exists a control input $u$ that takes us to $-x_1$ because the system is completely reachable.}

Hence
\[
\Phi(t_1, t_0)x_0 + \Gamma(u) = 0.
\]
Therefore
\[
\Phi(t_1, t_0)x_0 + \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) u(\tau) \, d\tau = 0.
\]
\textcolor{blue}{Controllability is also about taking the system from $x_0$ to $0$ in finite time. There exists a control to take it to 0.}

\subsection{If completely controlled, then completely reachable}  
$x_0$ is controllable. Every state $x_0 \in \mathbb{R}^n$ is controllable, hence the system is completely controllable.

Next, suppose that the system is completely controllable. For every $x_0 \in \mathbb{R}^n$ there exists $u \in U$ such that
\[
\Phi(t_1, t_0)x_0 + \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) u(\tau) \, d\tau = 0.
\]
Hence
\[
-\Phi(t_1, t_0)x_0 \in \text{R}(\Gamma).
\]
\textcolor{blue}{Take $\Phi(t_1,t_0)x_0$ to the right to get this.}

Given $x_1 \in \mathbb{R}^n$, let
\[
x_0 := -\Phi(t_0, t_1)x_1.
\]
\textcolor{blue}{$\Phi(t_0,t_1)x_1$ represents evolution from state $x_1$ from time $t_1$ to $t_0$. The minus sign is to take it to the origin.}

There exists $u \in U$ such that
\[
\Gamma(u) = x_1.
\]
Thus, $x_1 \in \text{R}(\Gamma)$. Therefore, $\text{R}(\Gamma) = \mathbb{R}^n$, hence the system is completely reachable.

\subsection{Controllability Grammian}
A finite-dimensional characterization of the reachable subspace is provided by the following $n \times n$ matrix, $G_c(t_0, t_1)$, called the controllability Grammian matrix:
\[
G_c(t_0, t_1) := \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) B(\tau)^T \Phi(t_1, \tau)^T \, d\tau
\]
The range of the matrix $G_c(t_0, t_1)$, denoted by $\text{R}(G_c(t_0, t_1))$, is
\[
\text{R}(G_c(t_0, t_1)) := \{ x \in \mathbb{R}^n : x = G_c(t_0, t_1) z \text{ for some } z \in \mathbb{R}^n \}.
\]
\textcolor{blue}{The controllability Grammian measures the amount of energy required by the input u(t) to move the system from the origin to a certain state over the interval. A larger Grammian implies that less energy is needed, indicating better controllability.}

\textcolor{blue}{$R(\Gamma)=R(G_c(t_0,t_1))$. This means that the controllability Grammian is a measure of the reachable subspace. The column space of the controllability Grammian is the reachable subspace. }    
 
\textcolor{blue}{If $G_c(t_0, t_1)$ has a large value in some directions, it means that, over the time interval $[t_0, t_1]$, the inputs can push the system strongly in those directions, making those states easily reachable. Conversely, if certain directions are associated with small values in $G_c(t_0, t_1)$, it indicates that, no matter what inputs we apply, those directions are harder to reach, meaning the system isn't very controllable in those dimensions.}

\textbf{Orthogonal Subspaces:}
The orthogonal subspace of $V_1$, denoted by $V_1^\perp$, is
\[
V_1^\perp := \{ z \in \mathbb{R}^n : \langle x, z \rangle = 0 \quad \forall x \in V_1 \}
\]

Observations:
\begin{enumerate}
    \item $(V_1^\perp)^\perp = V_1$
    \item Given two subspaces $V_1$ and $V_2$ of $\mathbb{R}^n$ such that $V_1 \subseteq V_2$, then $V_2^\perp \subseteq V_1^\perp$.
\end{enumerate}

\subsection{Proof of Proposition 19}
First, consider $x \in \text{R}(G_c(t_0, t_1))$.
There exists $z \in \mathbb{R}^n$ such that
\[
G_c(t_0, t_1)z = x
\]

\textcolor{blue}{For a given $x$ in the reachable subspace, there exists some $z$ such that $G_c(t_0, t_1)z = x$. This implies that $x$ can be achieved by applying a control input (related to $z$) over time, guided by the system's dynamics.}

\[
\left( \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) B(\tau)^T \Phi(t_1, \tau)^T \, d\tau \right) z = x
\]
\[
\int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) \left( B(\tau)^T \Phi(t_1, \tau)^T z \right) \, d\tau = x
\]
Define
\[
u(\tau) = B(\tau)^T \Phi(t_1, \tau)^T z.
\]
Then,
\[
\Gamma(u) = x.
\]
Thus,
\[
\text{R}(G_c(t_0, t_1)) \subseteq \text{R}(\Gamma).
\]

Next, we wish to prove that
\[
\text{R}(\Gamma) \subseteq \text{R}(G_c(t_0, t_1)).
\]
We do that by proving that
\[
\text{R}(G_c(t_0, t_1))^\perp \subseteq \text{R}(\Gamma)^\perp.
\]
Let $y \in \text{R}(G_c(t_0, t_1))^\perp$. Then
\[
y^T z = 0 \quad \forall z \in \text{R}(G_c(t_0, t_1)).
\]
Hence
\[
y^T G_c(t_0, t_1) x = 0 \quad \forall x \in \mathbb{R}^n.
\]
In particular, take $x = y$.
\[
y^T \left( \int_{t_0}^{t_1} \Phi(t_1, \tau) B(\tau) B(\tau)^T \Phi(t_1, \tau)^T \, d\tau \right) y = 0
\]
\[
\int_{t_0}^{t_1} y^T \Phi(t_1, \tau) B(\tau) B(\tau)^T \Phi(t_1, \tau)^T y \, d\tau = 0
\]
\[
\int_{t_0}^{t_1} \| B(\tau)^T \Phi(t_1, \tau)^T y \|^2 \, d\tau = 0
\]
The integrand in the latter integral, as a function of $\tau \in [t_0, t_1]$, is continuous, non-negative-valued, and its integral is 0.
\[
\| B(\tau)^T \Phi(t_1, \tau)^T y \|^2 \equiv 0
\]
\[
B(\tau)^T \Phi(t_1, \tau)^T y \equiv 0
\]
\[
y^T \Phi(t_1, \tau) B(\tau) \equiv 0
\]
Thus,
\[
y^T \Gamma(u) = 0 \quad \forall u \in U.
\]
\textcolor{blue}{Multiply $u(\tau)$ in the above equation and integrate over the interval to get this.}

Therefore,
\[
y \in \text{R}(\Gamma)^\perp.
\]
Hence,
\[
\text{R}(G_c(t_0, t_1))^\perp \subseteq \text{R}(\Gamma)^\perp.
\]
Taking orthogonal complements on both sides, we get
\[
\text{R}(\Gamma) \subseteq \text{R}(G_c(t_0, t_1)).
\]

\section{Time Invariant Case}
For a time-invariant system, the state-space representation is given by:
\[
\dot{x}(t) = Ax(t) + Bu(t)
\]

The controllability Grammian for the time-invariant case is:
\[
G_c(t_0, t_1) = \int_{t_1}^{t_0} e^{A(t_1 - \tau)} B B^T e^{A^T(t_1 - \tau)} \, d\tau
\]

The range of the controllability Grammian $G_c(t_0, t_1)$ is:
\[
\text{R}(\Gamma) = \text{R}(G_c(t_0, t_1))
\]

\subsection{Controllability Matrix}
Define the matrix $W_c \in \mathbb{R}^{n \times kn}$ by
\[
W_c = \begin{bmatrix}
B & AB & A^2B & \cdots & A^{n-1}B
\end{bmatrix}
\]
$W_c$ is called the controllability matrix. R($W_c$) is the reachable subspace.

\subsection{Proof of Proposition 20}
Proposition 20 states that $\text{R}(W_c)$ is the reachable subspace.

Proof: Since the reachable space is $\text{R}(G_c(t_0, t_1))$, we will prove that $\text{R}(G_c(t_0, t_1)) = \text{R}(W_c)$.

We do that by proving that $\text{R}(G_c(t_0, t_1))^\perp = \text{R}(W_c)^\perp$.

First, suppose that $y \in \text{R}(G_c(t_0, t_1))^\perp$. Then
\[
y^T G_c(t_0, t_1) x = 0 \quad \forall x \in \mathbb{R}^n.
\]
Take $x = y$, to obtain
\[
y^T G_c(t_0, t_1) y = 0
\]
\[
\int_{t_0}^{t_1} y^T e^{A(t_1 - \tau)} B B^T e^{A^T(t_1 - \tau)} y \, d\tau = 0.
\]
\[
\int_{t_0}^{t_1} \| B^T e^{A^T(t_1 - \tau)} y \|^2 \, d\tau = 0.
\]
The integrand in the latter integral, as a function of $\tau \in [t_0, t_1]$, is continuous, non-negative-valued, and its integral is 0.
\[
\| B^T e^{A^T(t_1 - \tau)} y \|^2 \equiv 0.
\]
\[
B^T e^{A^T(t_1 - \tau)} y \equiv 0.
\]
\[
y^T e^{A(t_1 - \tau)} B \equiv 0.
\]
Taking derivatives of the equation $y^T e^{A(t_1 - \tau)} B \equiv 0$ with respect to $\tau$, we get:
\[
\frac{d}{d\tau} \left( -y^T e^{A(t_1 - \tau)} B \right) \equiv 0,
\]
\[
\frac{d^2}{d\tau^2} \left( y^T e^{A(t_1 - \tau)} B \right) \equiv 0,
\]
\[
\vdots
\]
\[
\frac{d^{n-1}}{d\tau^{n-1}} \left( (-1)^{n-1} y^T A^{n-1} e^{A(t_1-\tau)} B \right) \equiv 0.
\]
Evaluating these derivatives at $\tau = t_1$, we obtain:
\[
y^T A^i B = 0, \quad i = 0, \ldots, n-1.
\]
Thus,
\[
y^T \begin{bmatrix}
B & AB & \cdots & A^{n-1}B
\end{bmatrix} = \begin{bmatrix}
0 & 0 & \cdots & 0
\end{bmatrix}.
\]
For every $z \in \mathbb{R}^n$,
\[
W_c z = \begin{bmatrix}
B & AB & \cdots & A^{n-1}B
\end{bmatrix} z.
\]
\[
y^T W_c z = y^T \begin{bmatrix}
B & AB & \cdots & A^{n-1}B
\end{bmatrix} z = 0.
\]
Thus,
\[
y \in \text{R}(W_c)^\perp.
\]
Hence,
\[
\text{R}(G_c(t_0, t_1))^\perp \subseteq \text{R}(W_c)^\perp.
\]
Taking orthogonal complements on both sides, we get
\[
\text{R}(W_c) \subseteq \text{R}(G_c(t_0, t_1)).
\]

Next, suppose that $y \in \text{R}(W_c)^\perp$. Then
\[
y^T W_c = \begin{bmatrix}
0 & 0 & \cdots & 0
\end{bmatrix}.
\]
Thus,
\[
y^T \begin{bmatrix}
B & AB & \cdots & A^{n-1}B
\end{bmatrix} = \begin{bmatrix}
0 & 0 & \cdots & 0
\end{bmatrix}.
\]
Hence,
\[
y^T e^{A(t_1 - \tau)} B = 0 \quad \forall \tau \in [t_0, t_1].
\]

\textcolor{blue}{
This expression represents a continuous-time version of the orthogonality condition over the interval $[t_0, t_1]$. 
In continuous systems, the state transition matrix $e^{A(t_1 - \tau)}$ generalizes the powers of $A$ seen in the controllability matrix 
(i.e., $B, AB, A^2B, \ldots, A^{n-1}B$). 
}
Thus,
\[
y^T \left( \int_{t_0}^{t_1} e^{A(t_1 - \tau)} B B^T e^{A^T(t_1 - \tau)} \, d\tau \right) y = 0
\]
\[
\int_{t_0}^{t_1} y^T e^{A(t_1 - \tau)} B B^T e^{A^T(t_1 - \tau)} y \, d\tau = 0
\]
Thus,
\[
y^T G_c(t_0, t_1) = 0.
\]
Hence,
\[
y \in \text{R}(G_c(t_0, t_1))^\perp.
\]
Thus,
\[
\text{R}(W_c)^\perp \subseteq \text{R}(G_c(t_0, t_1))^\perp.
\]
Taking orthogonal complements on both sides, we get
\[
\text{R}(G_c(t_0, t_1)) \subseteq \text{R}(W_c).
\]
Therefore,
\[
\text{R}(G_c(t_0, t_1)) = \text{R}(W_c).
\]

\subsection{Cayley-Hamilton Theorem}
The Cayley-Hamilton theorem states that every square matrix satisfies its own characteristic equation. 

For an $n \times n$ matrix $A$, the characteristic polynomial $p(\lambda)$ is defined as:
\[
p(\lambda) = \det(\lambda I - A)
\]
where $I$ is the identity matrix of the same dimension as $A$.

The Cayley-Hamilton theorem asserts that if we replace $\lambda$ with the matrix $A$ in the characteristic polynomial, the resulting matrix polynomial is the zero matrix:
\[
p(A) = 0
\]

To illustrate, consider a $2 \times 2$ matrix $A$:
\[
A = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\]
The characteristic polynomial is:
\[
p(\lambda) = \det(\lambda I - A) = \lambda^2 - (\text{tr}(A))\lambda + \det(A)
\]
where $\text{tr}(A) = a + d$ is the trace of $A$ and $\det(A) = ad - bc$ is the determinant of $A$.

According to the Cayley-Hamilton theorem:
\[
A^2 - (\text{tr}(A))A + \det(A)I = 0
\]

\subsection{Example: Series RLC Circuit}

Consider the series RLC circuit discussed in class. Let $x = \begin{pmatrix} i \\ v_c \end{pmatrix}$, and $u = v_{in}$.

\[
L \frac{di}{dt} = v_{in} - Ri - v_c
\]
\[
C \frac{dv_c}{dt} = i
\]

\[
\dot{x} = Ax + Bu
\]
\[
\dot{x}_1 = -\frac{R}{L} i - \frac{1}{L} x_2 + \frac{1}{L} u
\]
\[
\dot{x}_2 = \frac{1}{C} x_1
\]

\[
A = \begin{pmatrix}
-\frac{R}{L} & -\frac{1}{L} \\
\frac{1}{C} & 0
\end{pmatrix}
\]
\[
B = \begin{pmatrix}
\frac{1}{L} \\
0
\end{pmatrix}
\]

The modes:
\[
\lambda = \frac{1}{2} \left( -\frac{R}{L} \pm \sqrt{\frac{R^2}{L^2} - \frac{4}{LC}} \right)
\]

1. \( R^2 \geq \frac{4L}{C} \): Both eigenvalues are real and negative.

2. \( 0 < R^2 < \frac{4L}{C} \):
\[
\lambda = \frac{1}{2} \left( -\frac{R}{L} \pm j \sqrt{\frac{4}{LC} - \frac{R^2}{L^2}} \right)
\]
3. \( R = 0 \):
\[
\lambda = \pm j \frac{1}{\sqrt{LC}}
\]

Controllability:
\[
W_c = \begin{pmatrix}
B & AB
\end{pmatrix}
= \begin{pmatrix}
\frac{1}{L} & -\frac{R}{L^2} \\
0 & \frac{1}{LC}
\end{pmatrix}
\]
Completely controllable.

\subsection{System Linearization}
Consider the system
\[
\dot{x} = f(x, u)
\]
A small-signal analysis whereby \( u \rightarrow u + v \) and \( x \rightarrow x + \xi \) gives
\[
\dot{x} + \dot{\xi} = f(x + \xi, u + v) = f(x, u) + \frac{\partial f}{\partial x}(x, u)\xi + \frac{\partial f}{\partial u}(x, u)v
\]
Thus, the small-signal analysis is characterized by the linearized system
\[
\dot{\xi} = \frac{\partial f}{\partial x}(x, u)\xi + \frac{\partial f}{\partial u}(x, u)v.
\]

\section{Feedback Design}

\subsection{State Feedback Control}
Consider the system:
\[
\dot{x} = Ax + Bu
\]

The feedback law is given by:
\[
u = -Kx
\]

Substituting the feedback law into the system dynamics, we get the closed-loop system:
\[
\dot{x} = (A - BK)x
\]

\subsection{Design Objective: Eigenvalue Assignment}
The design objective is to assign the eigenvalues of the closed-loop system matrix $(A - BK)$ to desired locations in the complex plane. This is known as eigenvalue assignment.

\subsection{Controllability and Eigenvalue Assignment}
Is it always possible to assign the eigenvalues arbitrarily? The answer is yes, if and only if the system is completely controllable. If the system is not completely controllable, only some of the eigenvalues can be assigned.

\subsection{Feedback Design with rank($W_c)<n$}

\textbf{Similarity Transformation}

Consider the system:
\[
\dot{x} = Ax + Bu
\]

Let
\[
z := V^{-1}x.
\]

Then,
\[
\dot{z} = V^{-1}\dot{x} = V^{-1}(Ax + Bu) = V^{-1}AVz + V^{-1}Bu.
\]

Define
\[
\bar{A} = V^{-1}AV \quad \text{and} \quad \bar{B} = V^{-1}B.
\]

Then,
\[
\dot{z} = \bar{A}z + \bar{B}u
\]
with
\[
\bar{A} = V^{-1}AV, \quad \bar{B} = V^{-1}B.
\]

The eigenvalues of $A$ and $\bar{A}$ are identical.

\textbf{System Decomposition:}

\[ \dot{x} = Ax + Bu \]

Suppose that the system is not completely controllable. Let
\[
\dim(\text{R}(W_c)) = n_1 < n.
\]

\textbf{Proposition 21.} \(\text{R}(W_c)\) is invariant under \(A\).

Let \( x \in \text{R}(W_c) \). There exists \( z \in \mathbb{R}^{nk} \) such that
\[
x = W_c z.
\]
Let
\[
z^T = (z_0^T, \ldots, z_{n-1}^T).
\]
Then,
\[
x = W_c z = \begin{pmatrix} B & AB & \cdots & A^{n-1}B \end{pmatrix} \begin{pmatrix} z_0 \\ z_1 \\ \vdots \\ z_{n-1} \end{pmatrix} = \sum_{i=0}^{n-1} A^i B z_i.
\]
Now, consider \( Ax \):
\[
Ax = A \left( \sum_{i=0}^{n-1} A^i B z_i \right) = \sum_{i=0}^{n-1} A^{i+1} B z_i = \sum_{i=1}^{n} A^i B z_{i-1}.
\]
Reindexing the sum, we get:
\[
Ax = \sum_{i=1}^{n-1} A^i B z_{i-1} + A^n B z_{n-1}.
\]
Since \( A^n B z_{n-1} \) can be expressed as a linear combination of \( B, AB, \ldots, A^{n-1}B \), we have:
\[
Ax = \sum_{i=1}^{n-1} A^i B z_{i-1} + \sum_{i=0}^{n-1} \alpha_i A^i B z_{n-1} = \alpha_0 B z_{n-1} + \sum_{i=1}^{n-1} A^i (B z_{i-1} + \alpha_i z_{n-1}) \in \text{R}(W_c).
\]
Thus, \( \text{R}(W_c) \) is invariant under \( A \).

\textbf{Decomposition:}

Choose a basis for $\text{R}(W_c)$, $v_1, \ldots, v_{n_1}$.

Supplement it to a basis for $\mathbb{R}^n$, $v_1, \ldots, v_{n_1}, v_{n_1+1}, \ldots, v_n$.

\[
\bar{A} = V^{-1}AV = \begin{pmatrix}
A_{1,1} & A_{1,2} \\
0 & A_{2,2}
\end{pmatrix}
\]
and
\[
\bar{B} = V^{-1}B = \begin{pmatrix}
B_1 \\
0
\end{pmatrix}.
\]

\textbf{Design for the $\bar{A}, \bar{B}$ System}

The feedback law is given by:
\[
u = -\bar{K}z
\]
where
\[
\bar{K} = \begin{pmatrix}
K_1 & K_2
\end{pmatrix}.
\]

Substituting the feedback law into the system dynamics, we get the closed-loop system:
\[
\dot{z} = (\bar{A} - \bar{B}\bar{K})z.
\]

Thus,
\[
\bar{A} - \bar{B}\bar{K} = \begin{pmatrix}
A_{1,1} & A_{1,2} \\
0 & A_{2,2}
\end{pmatrix} - \begin{pmatrix}
B_1 \\
0
\end{pmatrix} \begin{pmatrix}
K_1 & K_2
\end{pmatrix}.
\]

This simplifies to:
\[
\bar{A} - \bar{B}\bar{K} = \begin{pmatrix}
A_{1,1} - B_1K_1 & A_{1,2} - B_1K_2 \\
0 & A_{2,2}
\end{pmatrix}.
\]

\textbf{Convert the design to the $\bar{A}, \bar{B}$ system:}

Define
\[
K = \bar{K}V^{-1}.
\]

Then
\[
\bar{A} - \bar{B}\bar{K} = V^{-1}AV - V^{-1}B\bar{K}
= V^{-1}AV - V^{-1}BVK = V^{-1}(A - BK)V.
\]

\section{Observability}

\subsection{State-dependent control vs. input-dependent control}

Consider the system:
\[
\dot{x}_1 = x_2
\]
\[
\dot{x}_2 = u
\]
\[
\dot{x} = Ax + bu
\]
where
\[
A = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}, \quad b = \begin{pmatrix}
0 \\
1
\end{pmatrix}.
\]

The controllability matrix $W_c$ is:
\[
W_c = \begin{pmatrix}
0 & 1 \\
1 & 0
\end{pmatrix}.
\]
\textcolor{blue}{The system is completely controllable because the controllability matrix $W_c$ has full rank.}

Next, suppose that we can measure only $x_2$. The system equations are:
\[
\dot{x}_1 = x_2
\]
\[
\dot{x}_2 = u
\]
\[
y = x_2
\]
\[
\dot{x} = Ax + bu, \quad y = cx
\]
where
\[
A = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}, \quad b = \begin{pmatrix}
0 \\
1
\end{pmatrix}, \quad c = \begin{pmatrix}
0 & 1
\end{pmatrix}.
\]

The control input is given by:
\[
u = -ky + v
\]
Substituting the control input into the system dynamics, we get:
\[
\dot{x} = Ax - bkcx + bv = (A - bkc)x + bv
\]
\[
= \begin{pmatrix}
0 & 1 \\
0 & -k
\end{pmatrix} x + \begin{pmatrix}
0 \\
1
\end{pmatrix} v.
\]

Thus, the system equations become:
\[
\dot{x}_1 = x_2
\]
\[
\dot{x}_2 = -kx_2 + v
\]
\[
x_1(t) = x_{1,0} + \int_0^t x_2(\tau) \, d\tau
\]

\textcolor{blue}{In this system, $x_1$ evolves based on the integral of $x_2$, and $x_2$ is directly influenced by the control input $v$. The feedback control law $u = -ky + v$ adjusts $x_2$ based on the measurement $y = x_2$, ensuring that the system's behavior is regulated by the control input $v$.}

\section{Observability}

Consider the general case:
\[
\dot{x}(t) = A(t)x(t) + B(t)u(t)
\]
\[
y(t) = C(t)x(t).
\]
Let \( t \in I := [t_0, t_1] \).

\textbf{Definition 28.} \( x_0 \in \mathbb{R}^n \) is unobservable on \( I \) if, with \( x(t_0) = x_0 \),
\[
y(t) \equiv 0 \quad \text{on } I.
\]

\textbf{Definition 29.} The system is completely observable on \( I \) if the only unobservable state \( x_0 \in \mathbb{R}^n \) is \( x_0 = 0 \).

Note that
\[
y(t) = C(t)\Phi(t, t_0)x_0
\]

Define the mapping
\[
\Lambda : \mathbb{R}^n \rightarrow Y \text{ as follows,}
\]
\[
\Lambda(x) = C(t)\Phi(t, t_0)x.
\]

Define the null space of \(\Lambda\) by
\[
N(\Lambda) = \{ x \in \mathbb{R}^n : \Lambda(x) \equiv 0 \text{ on } I \}
\]

\(N(\Lambda)\) is called the unobservable subspace.

Note that the system is completely observable iff
\[
N(\Lambda) = \{ 0 \}
\]

\subsection{Observability Grammian}

The observability Grammian for the time-varying system over the interval \( [t_0, t_1] \) is defined as:
\[
W_o(t_0, t_1) := \int_{t_0}^{t_1} \Phi(t, t_0)^T C(t)^T C(t) \Phi(t, t_0) \, dt
\]
where \( \Phi(t, t_0) \) is the state transition matrix.

\textcolor{blue}{The observability Grammian \( W_o(t_0, t_1) \) provides a measure of how well the states of the system can be inferred from the output over the interval \( [t_0, t_1] \). If \( W_o(t_0, t_1) \) is non-singular, the system is completely observable on \( [t_0, t_1] \).}

\subsection{Observability Matrix}

For a time-invariant system, the observability matrix \( \mathcal{O} \) is given by:
\[
\mathcal{O} = \begin{pmatrix}
C \\
CA \\
CA^2 \\
\vdots \\
CA^{n-1}
\end{pmatrix}
\]

\subsection{Example}

Consider the system:
\[
\dot{x} = Ax + bu, \quad y = cx
\]
where
\[
A = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}, \quad c = \begin{pmatrix}
0 & 1
\end{pmatrix}.
\]

The observability Grammian \( W_o \) is:
\[
W_o = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}.
\]

To check observability, we compute \( W_o x \):
\[
W_o x = 0.
\]

This implies that \( x_2 = 0 \).

The unobservable subspace is spanned by \( \begin{pmatrix} 1 \\ 0 \end{pmatrix} \).

\textcolor{blue}{The observability matrix \( \mathcal{O} \) collects the output matrices \( C \), \( CA \), \( CA^2 \), etc., up to \( CA^{n-1} \). If \( \mathcal{O} \) has full rank (i.e., rank \( n \)), the system is completely observable. This means that the current state of the system can be determined from the output measurements over a finite time interval.}

\section{State Feedback via an Observer}
\subsection{Feedback Design with Estimated Error from Observations}

\textcolor{blue}{Let $x$ be the state vector, $u$ be the control input, $v$ be the reference input, $K$ be the state feedback gain, $A$ be the system matrix, $B$ be the input matrix, $\bar{x}$ be the estimated state, and $e$ be the estimation error.}

Consider the system defined by the following equations:
\[
\dot{x} = Ax + Bu
\]
\[
\dot{\bar{x}} = A\bar{x} + Bu
\]
\[
u = v - K\bar{x}
\]
\[
e := x - \bar{x}
\]
\[
\dot{e} = Ae
\]
\[
\dot{x} = Ax + B(v - K\bar{x})
\]
\[
= A(x) + Bv - BK(x - e)
\]
\[
= (A - BK)x + BKe + Bv
\]
\[
\dot{x} = (A - BK)x + BKe + Bv
\]
\[
\dot{e} = Ae
\]
To control the convergence rate of the error to 0, we add the feedback loop to the system, as follows.

\[
\begin{pmatrix}
\dot{x} \\
\dot{e}
\end{pmatrix}
=
\begin{pmatrix}
A - BK & BK \\
0 & A
\end{pmatrix}
\begin{pmatrix}
x \\
e
\end{pmatrix}
+
\begin{pmatrix}
B \\
0
\end{pmatrix}
v
\]

This augmented system allows us to design the feedback gain $K$ to achieve the desired convergence rate for both the state $x$ and the estimation error $e$.

\subsection{Observer Feedback Design}
Consider the system defined by the following equations:
\[
\dot{x} = Ax + Bu
\]
\[
\dot{\bar{x}} = A\bar{x} + Bu + L(y - \bar{y})
\]
\[
u = v - K\bar{x}
\]
\[
e = x - \bar{x}
\]
\[
\dot{x} = Ax + B(v - K\bar{x})
\]
\[
\dot{\bar{x}} = A\bar{x} + LCe + B(v - K\bar{x})
\]
\[
e = x - \bar{x}
\]
\[
\dot{x} = Ax - BK(x - e) + Bv = (A - BK)x + BKe + Bv
\]
\[
\dot{e} = Ae - LCe = (A - LC)e
\]

\textcolor{blue}{
In this system, $\bar{x}$ is the estimated state, and $L$ is the observer gain. The estimation error $e = x - \bar{x}$ evolves according to $\dot{e} = (A - LC)e$. The control input $u = v - K\bar{x}$ is based on the estimated state $\bar{x}$.
}

\[
\begin{pmatrix}
\dot{x} \\
\dot{e}
\end{pmatrix}
=
\begin{pmatrix}
A - BK & BK \\
0 & A - LC
\end{pmatrix}
\begin{pmatrix}
x \\
e
\end{pmatrix}
+
\begin{pmatrix}
B \\
0
\end{pmatrix}
v
\]

\textcolor{blue}{
This augmented system allows us to design the feedback gain $K$ and the observer gain $L$ to achieve the desired convergence rate for both the state $x$ and the estimation error $e$. The matrix $\begin{pmatrix} A - BK & BK \\ 0 & A - LC \end{pmatrix}$ represents the dynamics of the combined system, ensuring that the state $x$ and the estimation error $e$ converge to their desired values.
}

\subsection{Example}

When the system is not completely observable, there exist states that cannot be inferred from the output measurements. This can be illustrated with the following example:

Consider the system:
\[
A = \begin{pmatrix}
A_{1,1} & 0 \\
A_{2,1} & A_{2,2}
\end{pmatrix}, \quad C = \begin{pmatrix}
C_1 & 0
\end{pmatrix}
\]

The observability matrix $\mathcal{O}$ is given by:
\[
\mathcal{O} = \begin{pmatrix}
C_1 & 0 \\
C_1 A_{1,1} & 0 \\
\vdots & \vdots \\
C_1 A_{1,1}^{n-1} & 0
\end{pmatrix}
\]

\textcolor{blue}{
In this case, the observability matrix $\mathcal{O}$ has a block structure where the second column is entirely zero. This indicates that the states associated with $A_{2,2}$ are not observable from the output $y = Cx$.
}

The null space of the observability Grammian $W_o$ is:
\[
N(W_o) = \{ z \in \mathbb{R}^n : W_o z = 0 \}
\]

Let
\[
z = \begin{pmatrix}
z_1 \\
z_2
\end{pmatrix}
\]

Since $C_1 A_{1,1}^i z_1 = 0$ for $i = 0, \ldots, n-1$, we have:
\[
z_1 = 0
\]

Thus,
\[
z = \begin{pmatrix}
0 \\
z_2
\end{pmatrix}
\]

\textcolor{blue}{
This implies that the unobservable subspace is spanned by the vectors associated with $z_2$. In other words, the states corresponding to $A_{2,2}$ cannot be observed from the output, making the system not completely observable.
}

What can we say about \(A - LC\)?

Let
\[
L = \begin{pmatrix}
L_1 \\
L_2
\end{pmatrix}.
\]

Then,
\[
A - LC = \begin{pmatrix}
A_{1,1} & 0 \\
A_{2,1} & A_{2,2}
\end{pmatrix} - \begin{pmatrix}
L_1 \\
L_2
\end{pmatrix} \begin{pmatrix}
C_1 & 0
\end{pmatrix}
= \begin{pmatrix}
A_{1,1} & 0 \\
A_{2,1} & A_{2,2}
\end{pmatrix} - \begin{pmatrix}
L_1 C_1 & 0 \\
L_2 C_1 & 0
\end{pmatrix}
= \begin{pmatrix}
A_{1,1} - L_1 C_1 & 0 \\
A_{2,1} - L_2 C_1 & A_{2,2}
\end{pmatrix}.
\]

\textcolor{blue}{This shows that the matrix \(A - LC\) has a block structure where the upper-left block is \(A_{1,1} - L_1 C_1\) and the lower-right block is \(A_{2,2}\). The off-diagonal block \(A_{2,1} - L_2 C_1\) represents the coupling between the observable and unobservable states.}

\section{Review of Similarity Transformation}

Consider the system:
\[
\dot{x} = Ax + Bu, \quad y = Cx
\]

Given a nonsingular matrix \( V \in \mathbb{R}^{n \times n} \), define:
\[
z = V^{-1}x
\]

Then,
\[
\dot{z} = V^{-1} \dot{x} = V^{-1}(Ax + Bu) = V^{-1}AVz + V^{-1}Bu, \quad y = Cx = CVz
\]

Thus, the transformed system is:
\[
\dot{z} = \bar{A}z + \bar{B}u, \quad \bar{y} = \bar{C}z
\]
with
\[
\bar{A} = V^{-1}AV, \quad \bar{B} = V^{-1}B, \quad \bar{C} = CV
\]

\textcolor{blue}{
Recall the following:
Let \( v_1, \ldots, v_n \) be a basis for \( \mathbb{R}^n \), and let \( V \) be the associated matrix.
Let \( x \in \mathbb{R}^n \) represent a coordinate-vector with respect to the \( v \)-basis.
Then \( y := Vx \) is the coordinate-vector with respect to the standard basis.
To see this, note that \( V e_i = v_i \), the \( i \)-th column of \( V \).
Now let a matrix \( A \) represent a transformation with respect to the standard basis. Then the
representation of \( A \) with respect to the \( v \)-basis is
\[
\bar{A} = V^{-1}AV.
\]
Next, consider \( B \in \mathbb{R}^{n \times k} \) representing a mapping from \( \mathbb{R}^k \) to \( \mathbb{R}^n \). Its representation with respect
to the \( v \)-basis in \( \mathbb{R}^n \) is
\[
\bar{B} = V^{-1}B;
\]
note that there is no change of basis in \( \mathbb{R}^k \).
Lastly, consider a matrix \( C \in \mathbb{R}^{m \times n} \) representing a mapping from \( \mathbb{R}^n \) to \( \mathbb{R}^m \). Then its representation with respect to the \( v \)-basis is
\[
\bar{C} = CV.
\]
}

\section{State-Space Realizations and the Kalman Decomposition}

Consider the following system,
\[
\dot{x} = Ax + Bu, \quad y = Cx,
\]
where \( x \in \mathbb{R}^n \), \( u \in \mathbb{R}^k \), and \( y \in \mathbb{R}^m \).
We denote it by an \((A, B, C)\) system.

\textcolor{blue}{
A state-space realization of a transfer function \( G(s) \) is a state-space system \((A, B, C)\) whose transfer function is \( G(s) \).
Two state-space realizations are equivalent if they have the same transfer function.
}

The transfer function of a state-space system \((A, B, C)\) can be determined by using Laplace transforms:
\[
sX(s) = AX(s) + BU(s)
\]
hence
\[
X(s) = (sI - A)^{-1}BU(s),
\]
and
\[
Y(s) = CX(s).
\]
Consequently, the transfer function \( G(s) \) is given by:
\[
G(s) = C(sI - A)^{-1}B.
\]

\textbf{Proposition 26}
Systems $(A, B, C)$ and $(\bar{A}, \bar{B}, \bar{C})$ that are related to each other by a similarity transformation are equivalent realizations.

\textbf{Proof.} Let
\[
\bar{A} = V^{-1}AV, \quad \bar{B} = V^{-1}B, \quad \bar{C} = CV.
\]
Then
\begin{align*}
\bar{G}(s) &:= \bar{C}(sI - \bar{A})^{-1}\bar{B} \\
&= CV (V^{-1}(sI - A)V)^{-1} V^{-1}B \\
&= CV (V^{-1}(sI - A)V)^{-1} V^{-1}B \\
&= CV V^{-1}(sI - A)^{-1} V V^{-1}B \\
&= C(sI - A)^{-1}B = G(s).
\end{align*}

Next, consider a system $(A, B, C)$ that is not completely controllable.
Let $n_1 < n$ be the dimension of $\text{R}(W_c)$. Suppose that the system is transformed by a similarity
transformation to an $(\bar{A}, \bar{B}, \bar{C})$ system where the first $n_1$ columns of $V$ are a basis for $\text{R}(W_c)$, and
\[
\bar{A} =
\begin{pmatrix}
A_{1,1} & A_{1,2} \\
0 & A_{2,2}
\end{pmatrix},
\quad
\bar{B} =
\begin{pmatrix}
B_1 \\
0
\end{pmatrix},
\quad
\bar{C} =
\begin{pmatrix}
C_1 & C_2
\end{pmatrix},
\]
where $A_{1,1} \in \mathbb{R}^{n_1 \times n_1}$, $B_1 \in \mathbb{R}^{n_1 \times k}$, $C_1 \in \mathbb{R}^{m \times n_1}$.

The transfer function is:
\[
G(s)
= \bar{C}(sI - \bar{A})^{-1}\bar{B}
= \begin{pmatrix}
C_1 & C_2
\end{pmatrix}
\begin{pmatrix}
sI - A_{1,1} & -A_{1,2} \\
0 & sI - A_{2,2}
\end{pmatrix}^{-1}
\begin{pmatrix}
B_1 \\
0
\end{pmatrix}
\]

\[
= \begin{pmatrix}
C_1 & C_2
\end{pmatrix}
\begin{pmatrix}
(sI - A_{1,1})^{-1} & (sI - A_{1,1})^{-1}A_{1,2}(sI - A_{2,2})^{-1} \\
0 & (sI - A_{2,2})^{-1}
\end{pmatrix}
\begin{pmatrix}
B_1 \\
0
\end{pmatrix}
\]

\[
= \begin{pmatrix}
C_1 & C_2
\end{pmatrix}
\begin{pmatrix}
(sI - A_{1,1})^{-1}B_1 \\
0
\end{pmatrix}
= C_1(sI - A_{1,1})^{-1}B_1.
\]

\textcolor{blue}{This is a reduced-order realization! The transfer function $G(s)$ depends only on the controllable part of the system, represented by $A_{1,1}$, $B_1$, and $C_1$. The uncontrollable part, represented by $A_{2,2}$ and $C_2$, does not affect the transfer function.}

\section{Kalman Decomposition}

In the case of controllability, the system matrices can be partitioned as follows:
\[
(A, B) \sim \left( \begin{pmatrix}
A_{1,1} & A_{1,2} \\
0 & A_{2,2}
\end{pmatrix}, \begin{pmatrix}
B_1 \\
0
\end{pmatrix} \right)
\]

In the case of observability, the system matrices can be partitioned as follows:
\[
(A, C) \sim \left( \begin{pmatrix}
A_{1,1} & 0 \\
A_{2,1} & A_{2,2}
\end{pmatrix}, \begin{pmatrix}
C_1 & 0
\end{pmatrix} \right)
\]

Putting them together:

Consider a system $(A, B, C)$. Denote by $G_{c,o}$ the intersection of the reachable subspace and the unobservable subspace. We next describe a procedure for constructing a basis for $\mathbb{R}^n$.

\begin{enumerate}
    \item Find a basis for $G_{c,o}$.
    \item Supplement the basis found in (1) to a basis for $G_c$.
    \item Supplement the basis found in (1) to a basis for $G_o$.
    \item Supplement the bases found in (1)-(3) to a basis for $\mathbb{R}^n$.
\end{enumerate}

Order the basis elements in the following way:
\[
\begin{pmatrix}
G_{c,o} & G_c \setminus G_{c,o} & G_o \setminus G_{c,o} & \mathbb{R}^n \setminus (G_c \cup G_o)
\end{pmatrix}
\]

Using this ordered basis, the system matrices $(A, B, C)$ can be transformed into the Kalman canonical form:
\[
A \sim \begin{pmatrix}
A_{c,o} & 0 & 0 & 0 \\
0 & A_{c,\bar{o}} & 0 & 0 \\
0 & 0 & A_{\bar{c},o} & 0 \\
0 & 0 & 0 & A_{\bar{c},\bar{o}}
\end{pmatrix}, \quad
B \sim \begin{pmatrix}
B_{c,o} \\
B_{c,\bar{o}} \\
0 \\
0
\end{pmatrix}, \quad
C \sim \begin{pmatrix}
C_{c,o} & 0 & C_{\bar{c},o} & 0
\end{pmatrix}
\]

Here, the subscripts $c$ and $o$ denote controllable and observable subspaces, respectively, and $\bar{c}$ and $\bar{o}$ denote their complements.

\subsection{Example of Kalman Decomposition}

Consider the following state-space system:

\[
A = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & -2 & -3
\end{pmatrix}, \quad
B = \begin{pmatrix}
0 \\
1 \\
0 \\
0
\end{pmatrix}, \quad
C = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0
\end{pmatrix}.
\]

We will perform the Kalman decomposition to separate the system into controllable/uncontrollable and observable/unobservable parts.

\subsubsection{Controllability Analysis}

First, compute the controllability matrix \( W_c \):

\[
W_c = \begin{bmatrix}
B & AB & A^2B & A^3B
\end{bmatrix} = \begin{bmatrix}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}.
\]

Compute the rank of \( W_c \):

\[
\text{rank}(W_c) = 2.
\]

Therefore, the controllable subspace has dimension 2.

\subsubsection{Observability Analysis}

Next, compute the observability matrix \( \mathcal{O} \):

\[
\mathcal{O} = \begin{bmatrix}
C \\
CA \\
CA^2 \\
CA^3
\end{bmatrix} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & -2 & -3 \\
0 & 0 & 0 & 0
\end{bmatrix}.
\]

Compute the rank of \( \mathcal{O} \):

\[
\text{rank}(\mathcal{O}) = 3.
\]

Therefore, the observable subspace has dimension 3.

\subsubsection{Kalman Decomposition}

We need to find a transformation matrix \( V \) that puts the system into Kalman canonical form.

Let us choose \( V \) such that:

\[
V = \begin{bmatrix}
v_1 & v_2 & v_3 & v_4
\end{bmatrix},
\]

where \( v_1, v_2 \) span the controllable subspace, and \( v_3, v_4 \) complete the basis.

Suppose the controllable subspace is spanned by:

\[
v_1 = \begin{pmatrix}
0 \\ 1 \\ 0 \\ 0
\end{pmatrix}, \quad
v_2 = \begin{pmatrix}
1 \\ 0 \\ 0 \\ 0
\end{pmatrix}.
\]

Let \( v_3 \) be chosen such that it spans the observable but uncontrollable subspace, for example:

\[
v_3 = \begin{pmatrix}
0 \\ 0 \\ 1 \\ 0
\end{pmatrix}.
\]

Let \( v_4 \) be the remaining vector to complete the basis, for example:

\[
v_4 = \begin{pmatrix}
0 \\ 0 \\ 0 \\ 1
\end{pmatrix}.
\]

Now, construct \( V \):

\[
V = \begin{pmatrix}
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}.
\]

Compute the transformed matrices:

\[
\bar{A} = V^{-1} A V, \quad \bar{B} = V^{-1} B, \quad \bar{C} = C V.
\]

After performing the calculations, we obtain:

\[
\bar{A} = \begin{pmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & -2 & -3
\end{pmatrix}, \quad
\bar{B} = \begin{pmatrix}
1 \\
0 \\
0 \\
0
\end{pmatrix}, \quad
\bar{C} = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{pmatrix}.
\]

This is the Kalman decomposition of the system, where:

- The controllable and observable subsystem is:

\[
A_{c,o} = \begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}, \quad
B_{c,o} = \begin{pmatrix}
1 \\
0
\end{pmatrix}, \quad
C_{c,o} = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}.
\]

- The uncontrollable but observable subsystem is:

\[
A_{\bar{c},o} = \begin{pmatrix}
0 & 1 \\
-2 & -3
\end{pmatrix}, \quad
B_{\bar{c},o} = \begin{pmatrix}
0 \\
0
\end{pmatrix}, \quad
C_{\bar{c},o} = \begin{pmatrix}
0 & 0 \\
1 & 0
\end{pmatrix}.
\]

- The uncontrollable and unobservable subsystem is empty.

Thus, the system has been decomposed into its controllable and observable parts.

\subsubsection{Interpretation}

The Kalman decomposition reveals that:

- The first two states are controllable and observable.
- The third and fourth states form an uncontrollable but observable subsystem.
- The system does not have any unobservable states that are also uncontrollable.

\subsubsection{Reduced-Order Model}

The transfer function of the system can be obtained from the controllable and observable subsystem:

\[
G(s) = \bar{C}(sI - \bar{A})^{-1} \bar{B} = C_{c,o}(sI - A_{c,o})^{-1} B_{c,o}.
\]

Calculating \( G(s) \):

\[
G(s) = \begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\left( sI - \begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix} \right)^{-1}
\begin{pmatrix}
1 \\
0
\end{pmatrix}
= \begin{pmatrix}
0 & 1
\end{pmatrix}
\begin{pmatrix}
\frac{1}{s} & 0 \\
0 & \frac{1}{s}
\end{pmatrix}
\begin{pmatrix}
1 \\
0
\end{pmatrix}
= \begin{pmatrix}
0 & \frac{1}{s}
\end{pmatrix}
\begin{pmatrix}
1 \\
0
\end{pmatrix}
= 0.
\]

Since the transfer function from the controllable and observable subsystem is zero, the output depends solely on the uncontrollable but observable subsystem.

This indicates that although some states are uncontrollable, they can still affect the output, which is important in system analysis.


\end{document}
